---
title: "Cross Validation"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
---

```{r}
#| label: setup
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(cowplot)
theme_set(theme_cowplot())
library(latex2exp)
library(wesanderson)
library(broom)
```

## The Problem

- Models maximize the amount of variation explained in the outcome variable by the predictor variables
- Can produce results specific to the data set (i.e., overfitting)
- More complex models = more specific, less general


## Simulation

Set up a data set where we know which variables are true predictors and which are not

- Draw outcome variable from a normal distribution
    - Two groups, differ in mean value
- Predictor 1 is the grouping variable 
- All other predictors are randomly generated and unrelated to the outcome variable


## Simulation

```{r}
#| echo: true

set.seed(29545)

nobs <- 10
yy <- c(rnorm(nobs / 2, 0, 1), rnorm(nobs / 2, 3, 1))
xx1 <- rep(c(0, 1), each = nobs / 2)
xx2 <- rnorm(nobs)
xx3 <- rnorm(nobs)
xx4 <- rnorm(nobs)
xx5 <- rnorm(nobs)
xx6 <- rnorm(nobs)
xx7 <- rnorm(nobs)
xx8 <- rnorm(nobs)
xx9 <- rnorm(nobs)
xx10 <- rnorm(nobs)
```


## Simulation

Fit 3 models:

```{r}
mod1 <- lm(yy ~ xx1)
mod2 <- lm(yy ~ xx1 + xx2 + xx3 + xx4 + xx5)
mod3 <- lm(yy ~ xx1 + xx2 + xx3 + xx4 + xx5 +
             xx6 + xx7 + xx8 + xx9 + xx10)
```

- Compare fitted values to assess how well the model fits the data  
- Remember $R^2$ = the squared correlation between the fitted and observed y values


## Model 1: xx1 only

```{r, echo=FALSE}
yy_hat1 <- mod1$fitted.values
data.frame('Yhat' = yy_hat1, 'Yobs' = yy) |>
  ggplot(aes(x = Yhat, y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("R^2 = ",
                          round(summary(mod1)$r.squared, 2),
                          "; Correlation = ",
                          round(sqrt(summary(mod1)$r.squared), 2))))
```


## Model 2: xx1 ... xx5

```{r, echo=FALSE}
yy_hat2 <- mod2$fitted.values
data.frame('Yhat' = yy_hat2, 'Yobs' = yy) |>
  ggplot(aes(x = Yhat, y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("R^2 = ",
                          round(summary(mod2)$r.squared, 2),
                          "; Correlation = ",
                          round(sqrt(summary(mod2)$r.squared), 2))))
```


## Model 3: xx1 ... xx10

```{r, echo=FALSE}
yy_hat3 <- mod3$fitted.values
data.frame('Yhat' = yy_hat3, 'Yobs' = yy) |>
  ggplot(aes(x = Yhat, y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("R^2 = ",
                          round(summary(mod3)$r.squared, 2),
                          "; Correlation = ",
                          round(sqrt(summary(mod3)$r.squared), 2))))
```


## Simulation

- How will our models perform with new data?

```{r}
#| echo: true

newN <- 10
newy <- c(rnorm(newN / 2, 0, 1), rnorm(newN / 2, 3, 1))
newx <- data.frame('xx1' = rep(c(0, 1), each = newN / 2),
                   'xx2' = rnorm(newN),
                   'xx3' = rnorm(newN),
                   'xx4' = rnorm(newN),
                   'xx5' = rnorm(newN),
                   'xx6' = rnorm(newN),
                   'xx7' = rnorm(newN),
                   'xx8' = rnorm(newN),
                   'xx9' = rnorm(newN),
                   'xx10' = rnorm(newN))

y_hat_new1 <- predict.lm(object = mod1, newdata = newx)
y_hat_new2 <- predict.lm(object = mod2, newdata = newx)
y_hat_new3 <- predict.lm(object = mod3, newdata = newx)
```


## Model 1: xx1 only

```{r}
miny <- min(c(newy, yy))
maxy <- max(c(newy, yy))

minx <- min(c(y_hat_new1, y_hat_new2, y_hat_new3,
              yy_hat1, yy_hat2, yy_hat3))
maxx <- max(c(y_hat_new1, y_hat_new2, y_hat_new3,
              yy_hat1, yy_hat2, yy_hat3))

new1 <- data.frame('Yhat' = y_hat_new1, 'Yobs' = newy)
new2 <- data.frame('Yhat' = y_hat_new2, 'Yobs' = newy)
new3 <- data.frame('Yhat' = y_hat_new3, 'Yobs' = newy)

data.frame('Yhat' = yy_hat1, 'Yobs' = yy) |>
  ggplot(aes(x = Yhat, y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  geom_point(data = new1, aes(x = Yhat, y = Yobs),
             size = 3, alpha = 1 / 3, color = 'red') +
  ylim(miny, maxy) +
  xlim(minx, maxx) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("Correlation (new data vs. predicted values) = ",
                          round(cor(y_hat_new1,newy), 2))))
```


## Model 2: xx1 ... xx5

```{r}
data.frame('Yhat' = yy_hat2, 'Yobs' = yy) |>
  ggplot(aes(x = Yhat, y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  geom_point(data = new2, aes(x = Yhat, y = Yobs),
             size = 3, alpha = 1 / 3, color = 'red') +
  ylim(miny, maxy) +
  xlim(minx, maxx) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("Correlation (new data vs. predicted values) = ",
                          round(cor(y_hat_new2,newy), 2))))
```


## Model 3: xx1 ... xx10

```{r}
data.frame('Yhat' = yy_hat3, 'Yobs' = yy) |>
  ggplot(aes(x = Yhat , y = Yobs)) +
  geom_point(size = 3, alpha = 1 / 3) +
  geom_point(data = new3, aes(x = Yhat, y = Yobs),
             size = 3, alpha = 1 / 3, color = 'red') +
  ylim(miny, maxy) +
  xlim(minx, maxx) +
  labs(x = TeX("$\\hat{Y}$"),
       title = TeX(paste0("Correlation (new data vs. predicted values) = ",
                          round(cor(y_hat_new3,newy), 2))))
```


## Cross Validation

> When we don't know the true model (all cases but simulation), how do we know if a model is too specific to a given data set?

- Cross validation is one technique to address this question


## Approach

- Split data into training and test sets
- Use training set to build model
- Use test set to evaluate model
- Repeat to get average error
- **Which Predictors?**
- **What Parameter Values?**


## Approaches

- Random sub samples as test set
    - Issue: reuse of data
- K-fold cross validation
    - Issue: need large data set
- Leave-one-out
    - Issue: small test set, can be slow


## Energy expenditure in naked mole rats

```{r}
M <- read_csv("../data/Molerats.csv", col_types = c("cdd")) |> 
  rename(Caste = caste,
         Mass = ln.mass,
         Energy= ln.energy) |> 
  mutate(Caste = if_else(Caste == "worker", "Worker", "Non-worker"),
         Caste = factor(Caste))

ggplot(M, aes(x = Mass, y = Energy, color = Caste)) +
  geom_point(size = 4) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0.05, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")
```


## Fit different models to these data

1. Categorical only (group mean, no body mass)
1. Mixed predictors, intercepts varying
1. Mixed predictors, slopes varying & intercepts varying

```{r}
mod1 <- lm(Energy ~ Caste, data = M)
mod2 <- lm(Energy ~ Mass + Caste, data = M)
mod3 <- lm(Energy ~ Mass * Caste, data = M)
```


## Fit different models to these data

```{r}
M_pred <- crossing(
  Mass = seq(3.8, 5.3, length = 100),
  Caste = levels(M$Caste))

M_pred <- M_pred |> 
  mutate(mod1 = predict(mod1, newdata = M_pred),
         mod2 = predict(mod2, newdata = M_pred),
         mod3 = predict(mod3, newdata = M_pred))

p <- ggplot() +
  geom_point(data = M, aes(x = Mass, y = Energy, color = Caste), size = 4,
             alpha = 0.5) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.position = "none") +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)")

p1 <- p +
  geom_line(data = M_pred, aes(x = Mass, y = mod1, color = Caste),
            linewidth = 1.5) +
  labs(title = "Caste")

p2 <- p +
  geom_line(data = M_pred, aes(x = Mass, y = mod2, color = Caste),
            linewidth = 1.5) +
  labs(title = "Caste + Mass")

p3 <- p +
  geom_line(data = M_pred, aes(x = Mass, y = mod3, color = Caste),
            linewidth = 1.5) +
  labs(title = "Caste * Mass")

plot_grid(p1, p2, p3, ncol = 3)
```


## Assess models with 10-fold cross validation

1. Split data into 10 ~equal sets
1. Use each set as the test set with the remaining data as the training set
1. Quantify error 
    - **Mean squared error**
    - Correlation between predicted & observed values
    - Categorical outcome: % misclassified


## 10-fold cross validation
    
```{r}
#| echo: true

set.seed(533214)
folds <- 10  # <1>

M$id_fold <- sample(cut(1:nrow(M), breaks = 10) |> # <2>
                      as.factor() |> 
                      as.integer())

err_out <- data.frame('mod1' = numeric(length = folds), # <3>
                      'mod2' = numeric(length = folds),
                      'mod3' = numeric(length = folds))
head(err_out)
```
1. Set the number of folds
2. Randomly assign each row to one of the folds.
3. Create a `data.frame` to hold the MSE values for each of the three models


## 10-fold cross validation
    
```{r}
#| echo: true

for(ii in 1:folds) {
  M_train <- M[M$id_fold != ii, ]  # <1>
  
  mod1 <- lm(Energy ~ Caste, data = M_train) # <2>
  mod2 <- lm(Energy ~ Mass + Caste, data = M_train)
  mod3 <- lm(Energy ~ Mass * Caste, data = M_train)
  
  M_test <- M[M$id_fold == ii, ] # <3>
  
  pp1 <- predict(mod1, M_test)  # <4>
  pp2 <- predict(mod2, M_test)
  pp3 <- predict(mod3, M_test)
  
  err_out[ii, 'mod1'] <- mean((M_test$Energy - pp1) ^ 2)  # <5>
  err_out[ii, 'mod2'] <- mean((M_test$Energy - pp2) ^ 2)
  err_out[ii, 'mod3'] <- mean((M_test$Energy - pp3) ^ 2)
}
```
1. Subset the data for the *ii*th fold to create the "training" set
2. Fit three linear models using the training set
3. Subset the data for the *ii*th fold to create the "test" set
4. Predict the test data for each of the three models
5. Assign the MSE to the `err_out` object


## Visualizing MSE of one "fold"

```{r}
preds <- M_test |> 
  select(-id_fold) |> 
  mutate(`Caste only` = pp1,
         `Mass + Caste` = pp2,
         `Mass * Caste` = pp3) |> 
  pivot_longer(cols = -c(Caste, Mass, Energy),
               names_to = "Model", values_to = "value")

ggplot() +
  geom_vline(xintercept = M_test$Mass, color = "gray60",
             linetype = "dotted") +
  geom_point(data = M_train, aes(x = Mass, y = Energy, color = Caste),
             size = 2) +
  scale_color_manual(values = wes_palette("Cavalcanti1")) +
  theme(legend.justification = c(0, 1),
        legend.position = c(0.05, 1)) +
  labs(x = "ln Body Mass (g)",
       y = "ln Daily Energy Expenditure (kJ)") +
  geom_point(data = M_test, aes(x = Mass, y = Energy, color = Caste),
             size = 4, pch = 16) +
  geom_point(data = preds, aes(x = Mass, y = value, color = Caste,
                               shape = Model), size = 6) +
  scale_shape_manual(values = c(0, 1, 2))
```


## 10-fold cross validation
    
```{r}
#| echo: true

err_out

colMeans(err_out)
```


## 10-fold cross validation

```{r}
err_out |>
  mutate(Fold = factor(1:folds)) |> 
  gather(Model, MSE, -Fold) |> 
  mutate(Model = factor(
    Model,
    labels = c("Caste only", "Mass + Caste", "Mass * Caste"))) |> 
  ggplot(aes(Model, MSE, color = Fold)) +
  geom_point(size = 3) +
  labs(y = "Mean Squared Error") +
  stat_summary(fun = "mean", geom = "point", color = "black",
               size = 6)
```


## Notes & Resources

- Training and Test sets must come from the same population
- There is uncertainty in cross validation estimates
- Custom code here but several R packages automate this process:
    - `cvTools` package
    - `boot` package
    - [tidymodels](https://www.tidymodels.org/) family of packages

