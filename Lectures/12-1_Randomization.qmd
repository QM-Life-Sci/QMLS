---
title: "Distribution Free Methods"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
---


## Distribution Free Methods

```{r}
#| label: setup
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(cowplot)
theme_set(theme_cowplot())
library(readxl)
library(gt)

```

Distribution free methods do not assume the data are drawn from any specific probability distribution. *Important: distribution-free, not assumption-free (e.g., IID).*

- Randomization 
    - Shuffle data set in some way
- Resampling
    - Sample from data set in some way
      - Jackknife
      - Bootstrap


## Why distribution free methods?      
  
  - Real data often does not follow standard distributions
  - Very flexible
  - Uses only information from your data set
      - Good & Bad
  - Standard errors can not always be calculated for all parameters

  
## Resampling

  - Primarily a method for parameter estimation and interval estimation
  - Sample from the data to estimate the variability in the parameter estimate
  - **Assumption:** Observed distribution is representative of the true distribution
  - Need to empirically show the method works for each type of application using simulation


## Randomization

  - Primarily a method for decision making (hypothesis testing)
  - More directly asks: What is the probability of observing a result as extreme or more than my observed result? 
  
  **Assumption:** Under the null hypothesis, observations are random draws from a common population


## Hypothesis testing by randomization

![](https://i.imgur.com/OOYvufC.jpg){fig-align="center"}


## Hypothesis testing by randomization

Mandible lengths of female and male jackals from the Natural History Museum (London).

```{r}
#| echo: true

M <- read_excel("../data/Jackals.xlsx") |> 
  mutate(Sex = factor(Sex))
glimpse(M)
```


## Randomization tests

```{r}
ggplot(M, aes(Mandible)) +
  geom_histogram(bins = 30) +
  facet_grid(Sex ~ .) +
  labs(x = "Mandible Length (mm)", y = "Count")
```


## Randomization tests

We'd like to do a linear model to compare group means.

But:

1. Assumes random sampling
2. Assumes equal group variances
3. Assumes normal distribution within groups


## Randomization procedure

1. Decide on a test statistic
1. Calculate the test statistic for the *observed* data
1. Randomly shuffle the observations
1. Calculate the test statistic for that group
1. Repeat 10^*x*^ of times (*x* is 3, 4, 5 or more)
1. Determine the proportion of random combinations resulting in a test statistic more extreme than the observed value ("empirical *P*")


## Randomization procedure

When test assumptions are met, the results will match asymptotic procedures (with enough iterations).

When the assumptions are not met, the results will be valid.


## Decide on a test statistic

1. Mean difference
2. *t*: *t*-test, linear model parameter estimate (slope, intercept)
3. *F*: ANOVA-like
4. $\chi^2$
5. Any metric of your choice (P-value, Fst, heterozygosity, LOD score, etc.)


## Calculate the test statistic for the observed data

Observed difference in mean value for females vs. males:

```{r}
#| echo: true

(obs <- mean(M$Mandible[M$Sex == "F"]) -
   mean(M$Mandible[M$Sex == "M"]))
```


## Randomly reassign, recalculate, repeat

```{r}
add_mean <- function(D) {
  F_mean <- D |> filter(Sex == "F") |> 
    summarize(bar = mean(Mandible)) |> pull()
  M_mean <- D |> filter(Sex == "M") |> 
    summarize(bar = mean(Mandible)) |> pull()
  st <- paste("F =", F_mean, "\nM =", M_mean)
  return(st)
}

M.set <- M |>
  mutate("Original"= M$Sex)


ggplot(M.set, aes(x = Sex, y = Mandible, color = Original)) +
  geom_point(position = position_jitter(width = 0.05),
             alpha = 1 / 2, size = 3) +
  annotate("text", x = 1.5, y = 115, label = add_mean(M.set),
           size = 7)
```


## Randomly reassign, recalculate, repeat

```{r}
set.seed(23784)

M.shuffle <- M.set |>
  mutate("Sex"= sample(M$Sex))

ggplot(M.shuffle, aes(x = Sex, y = Mandible, color = Original)) +
  geom_point(position = position_jitter(width = 0.05),
             alpha = 1 / 2, size = 3) +
  annotate("text", x = 1.5, y = 115, label = add_mean(M.shuffle),
           size = 7)
```


## Randomly reassign, recalculate, repeat

```{r}

M.shuffle <- M.set |>
  mutate("Sex"= sample(M$Sex))

ggplot(M.shuffle, aes(x = Sex, y = Mandible, color = Original)) +
  geom_point(position = position_jitter(width = 0.05),
             alpha = 1 / 2, size = 3) +
  annotate("text", x = 1.5, y = 115, label = add_mean(M.shuffle),
           size = 7)
```


## Randomly reassign, recalculate, repeat

```{r mc_mean}
#| echo: true

# Set random number seed
set.seed(10)

nreps <- 1e4
diffs <- numeric(length = nreps)
diffs[1] <- obs
for (ii in 2:nreps) {
  Rand_Sex <- sample(M$Sex)
  diffs[ii] <- mean(M$Mandible[Rand_Sex == "F"]) -
    mean(M$Mandible[Rand_Sex == "M"])
}
```


## Visualize the results

```{r}
#| echo: true
#| output-location: slide

ggplot(data.frame(diffs), aes(diffs)) +
  geom_histogram(bins = 25) +
  geom_vline(xintercept = diffs[1], color = "firebrick4",
             linewidth = 2) +
  labs(x = "Difference", y = "Count")
```

Histogram of the randomized differences, vertical red line marks the observed value.


## Proportion of randomized differences more extreme than the observed

```{r}
#| echo: true

mean(diffs <= diffs[1])
```

Mean of `diffs` where the value is *less than or equal to* the observed mean difference.

Empirically determined *P*-value is `r round(mean(diffs <= diffs[1]), 4)`.


## Example: Age predicted by coloration in lion noses

:::: {.columns}

::: {.column width="30%"}

![](https://i.imgur.com/beEXTFK.jpg){fig-align="center"}
:::

::: {.column width="70%"}

![](https://i.imgur.com/5kXcaVu.jpg){fig-align="center"}
:::

::::


## Age predicted by coloration in lion noses

```{r}
data("LionNoses", package = "abd")
ggplot(LionNoses, aes(proportion.black, age)) +
  geom_point(size = 4) +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x,
              linewidth = 2) +
  labs(x = "Proportion of Nose that is Black", y = "Age (y)")
```


## Age predicted by nose coloration

```{r}
#| echo: true

fm <- lm(age ~ proportion.black, data = LionNoses)
summary(fm)
```


## Age predicted by nose coloration

```{r}
set.seed(481769)
nreps <- 1e4
pb <- numeric(nreps)
pb[1] <- coef(fm)[2] |> as.numeric()

for (ii in 2:nreps) {
  Rand_Age <- sample(LionNoses$age)
  fm_rand <- lm(Rand_Age ~ proportion.black, data = LionNoses)
  pb[ii] <- coef(fm_rand)[2] |> as.numeric()
}

ggplot(data.frame(pb), aes(pb)) +
  geom_histogram(bins = 30) +
  geom_vline(xintercept = pb[1], color = "firebrick4",
             linewidth = 2) +
  labs(x = "Coefficient for Proportion Black", y = "Count")
```


## Age predicted by nose coloration

```{r}
#| echo: true

2 * mean(pb >= pb[1])
```

Note sign of the inequality: how often does the randomized age data give a *larger* *t* statistic than the observed?

Why 2 x 10^-4^?


## Empirical *P* vs. Iterations

What is the minimal detectable *P* for *n* iterations?

```{r}
#| output: asis

reps_ex <- data.frame(nreps = c(500, 1000, 5000, 10000, 50000))
reps_ex$min_P <- 2 * (1 / reps_ex$nreps)

reps_ex |> 
  gt() |> 
  cols_label(nreps = md("Number of iterations"),
             min_P = md("Minimum detectable *P*-value")) |> 
  tab_style(style = cell_text(weight = "bold"),
            locations = cells_column_labels()) |> 
  tab_style(
    style = cell_text(size = px(30)),
    locations = list(cells_column_labels(),
                     cells_body(
                       columns = everything()))) |> 
  as_raw_html()

```


## Non-parametric tests

Non-parametric tests often used when data do not meet the assumptions of a traditional (parametric) test:

- One-sample *t*-test $\rightarrow$ Sign test, Wilcoxon test
- Two-sample *t*-test $\rightarrow$ Mann-Whitney test
- ANOVA $\rightarrow$ Kruskal-Wallis

Small sample size, non-normality, unequal variances

**Dramatically lower power compared to a parametric test**


## Randomization as an alternative

For all practical cases, randomization is a better alternative

- Increased power
- No reliance on asymptotic properties of tests
- More relaxed assumptions

