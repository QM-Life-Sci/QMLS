---
title: "Composite Variables"
author:
  - Elizabeth King
  - Kevin Middleton
format:
  revealjs:
    theme: [default, custom.scss]
    standalone: true
    self-contained: true
    logo: QMLS_Logo.png
    slide-number: true
    show-slide-number: all
code-annotations: hover
bibliography: QMLS_Bibliography.bib
csl: evolution.csl
---


```{r}
#| label: setup
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(cowplot)
library(smatr)
library(factoextra)
library(readxl)
library(plotly)
library(GGally)
library(knitr)

theme_set(theme_cowplot())
```

<!--
Datasets
  Milk.xlsx
  mammals.xlsx
-->


## Motivation: the curse of dimensionality

Multivariate data is not just a challenge for visualization.

You have too many variables

- Some are redundant in measuring essentially the same thing
- Some are highly correlated with others (multicollinear)
- Some are both

Your number of variables approaches your number of observations.


## What is a composite variable?

> A linear combination of variables

Multiple Regression Makes a Composite Variable

- Linear combination of predictor variables that is maximally correlated with outcome variable
- $y = {\beta_1}X_1 + {\beta_2}X_2 + {\beta_3}X_3$
- $\hat{y}$  is a composite variable


## Multiple regression makes a composite variable

```{r echo=FALSE, warning = FALSE}
milk <- read_excel("../data/Milk.xlsx", na = "NA")
M <- milk %>% 
  select(species, kcal.per.g, perc.fat, perc.lactose) %>%
  drop_na() %>%
  as.data.frame()
names(M) <- c("Species", "Milk_Energy", "Fat", "Lactose")

ggscatmat(M)
```


## Milk investment across mammalian species

```{r webgl=TRUE, echo=FALSE}
fig <- plot_ly() %>%
  add_markers(data = M,
              x = ~ Fat,
              y = ~ Lactose,
              z = ~ Milk_Energy,
              size = 1,
              showlegend = FALSE) %>%
  hide_colorbar() %>%
  layout(scene = list(xaxis = list(title = 'Fat'),
                      yaxis = list(title = 'Lactose'),
                      zaxis = list(title = 'Milk Energy')))

fig
```


## Principal components analysis

*Goals*:

1. Create a set of composite variables that encompasses the shared variation among variables
1. Reduce the dimensionality of a set of variables: from considering all variables separately to considering fewer composite variables, while
1. Accounting for as much of the original variation as possible in the data set 

No predictor or outcome variable. Think of it as a one sided equation.


## Principal components

- Sequential linear combinations of the original variables 
- The resulting composite variables *are uncorrelated with one another*
- The first few usually account for *most* of the variation in the original data


## Principal components

For *q* variables, you get *q* PCs

- First encompasses the maximum variance
- Each in turn maximizes remaining variance while remaining uncorrelated


## Milk investment 

```{r echo=FALSE}
ggplot(M, aes(Fat, Lactose)) + geom_point() + coord_fixed()
```


## Milk investment 

Find the indices of the minimum and maximum of `x1`

```{r}
themin <- which.min(M$Fat)
themax <- which.max(M$Fat)
```

Make a `data.frame` with the points for plotting

```{r}
minpt <- data.frame(x1 = M$Fat[themin], x2 = M$Lactose[themin])
maxpt <- data.frame(x1 = M$Fat[themax], x2 = M$Lactose[themax])
```


## Milk investment 

Green = Low Fat, High Lactose

Blue = High Fat, Low Lactose

```{r echo=FALSE}
ma <- line.cis(M$Lactose, M$Fat,  method = "MA")

p <- ggplot(M, aes(Fat, Lactose)) + geom_point() +
  geom_point(data = minpt, aes(x1, x2), col = "green", size = 3) +
  geom_point(data = maxpt, aes(x1, x2), col = "blue", size = 3) +
  coord_fixed()
print(p)
```


## Major axis == PC1

Green = Low Fat, High Lactose

Blue = High Fat, Low Lactose

```{r echo=FALSE}
MA_plot <- p +
  geom_abline(slope = ma[2, 1], intercept = ma[1, 1],
              color = "red",
              size = 1.5)
print(MA_plot)
```


## Minor axis == PC2

Green = Low Fat, High Lactose

Blue = High Fat, Low Lactose

```{r echo=FALSE}
raw_plot <- MA_plot +
  geom_abline(slope = -1/ma[2, 1],
              intercept = mean(M$Lactose) - -1/ma[2, 1] * mean(M$Fat),
              color = "red",
              size = 1.5)
print(raw_plot)
```


## Plotting PCs

- Rotation of the axes of multivariate data 

Green = Low Fat, High Lactose

Blue = High Fat, Low Lactose

```{r echo=FALSE}
z <- prcomp(~ Fat + Lactose, data = M, center = TRUE, scale. = TRUE)
PC <- data.frame(pc1 = z$x[, 1],
                 pc2 = z$x[, 2])

minpt <- data.frame(x1 = PC$pc1[themin], x2 = PC$pc2[themin])
maxpt <- data.frame(x1 = PC$pc1[themax], x2 = PC$pc2[themax])

pc_plot <- ggplot(PC, aes(pc1, pc2)) +
  geom_vline(xintercept = 0, color = "red", size = 1.5) +
  geom_hline(yintercept = 0, color = "red", size = 1.5) +
  geom_point() +
  geom_point(data = minpt, aes(x1, x2), col = "green", size = 3) +
  geom_point(data = maxpt, aes(x1, x2), col = "blue", size = 3) +
  coord_fixed()
print(pc_plot)
```


## Comparing PCs

Green = Low Fat, High Lactose

Blue = High Fat, Low Lactose

```{r echo=FALSE}
plot_grid(raw_plot, pc_plot, ncol = 2)
```


## Principal components

`prcomp()` is the preferred function for PCA (*not* `princomp()`):

```{r}
z <- prcomp(~ Fat + Lactose, data = M, center = TRUE, scale. = TRUE)
```

- One sided formula: `~ Fat + Lactose`
- Centered to a mean of 0
- Scaled to standard deviation of 1


## `prcomp` objects {.smaller}

```{r}
str(z)
```

`x` is a matrix of the the principal components


## Proportion of variance (eigenvalues)

```{r}
summary(z)
```

PC1 explains 97% of the variance in the data.


## Loadings (eigenvectors)

Correlations with with composite variable

```{r}
print(z)
```

- `Lactose` loads negatively on PC1 and PC2
- `Fat` loads positively on PC1 and negatively on PC2
- Magnitudes are more informative with more than 2 variables


## Extracting PC scores

```{r}
PC <- data.frame(pc1 = z$x[, 1],
                 pc2 = z$x[, 2])
PC
```


## Extracting PC scores

Green = Low Fat, High Lactose

Blue = High Fat, Low Lactose

```{r echo=FALSE}
PC <- data.frame(pc1 = z$x[, 1],
                 pc2 = z$x[, 2])
PC[1,]
pc_plot <- ggplot(PC, aes(pc1, pc2)) +
  geom_vline(xintercept = 0, color = "red", size = 1.5) +
  geom_hline(yintercept = 0, color = "red", size = 1.5) +
  geom_point() +
  geom_point(data = minpt, aes(x1, x2), col = "green", size = 3) +
  geom_point(data = maxpt, aes(x1, x2), col = "blue", size = 3) +
  geom_point(aes(x= -1.785, y=-0.0636), col="cyan", size=3) +
  geom_segment(x = -1.785, xend = -1.785, y = -1, yend = -0.0636, col='cyan',lty=3,lwd=1.2) +
  geom_segment(x = -3, xend = -1.785, y = -0.0636, yend = -0.0636,col='cyan',lty=3,lwd=1.2) +
  coord_fixed()
print(pc_plot)

```


## Milk investment: PCA approach {.smaller}

1. Do fat and lactose together predict milk energy?

```{r}
PC1 <- z$x[, 1]
summary(lm(Milk_Energy ~ PC1, data = M))
```


## Milk investment: Multiple Regression {.smaller}

1. Do fat and lactose together predict milk energy?

```{r}
fm_Multi <- lm(Milk_Energy ~ Fat + Lactose, data = M)
summary(fm_Multi)
```


## Mammal life history

```{r}
M <- read_excel("../data/mammals.xlsx", na = "NA") %>%
  dplyr::select(litter_size,
                adult_body_mass_g,
                neonate_body_mass_g,
                max_longevity_m,
                sexual_maturity_age_d) %>% 
  rename(Litter_Size = litter_size,
         Adult_Mass = adult_body_mass_g,
         Neonate_Mass = neonate_body_mass_g,
         Longevity = max_longevity_m,
         Maturity_Age = sexual_maturity_age_d) %>% 
  drop_na()
```


## Mammal life history

`~ .` means all columns

```{r}
z <- prcomp(~ .,
            data = M,
            center = TRUE,
            scale. = TRUE)
```

Centering and $Z$-scaling are important.

- Should be the default but are not.


## Use `factoextra` for more handy functions

```{r eval=FALSE}
remotes::install_github("kassambara/factoextra")
library(factoextra)
```

[factoextra](http://www.sthda.com/english/wiki/factoextra-r-package-quick-multivariate-data-analysis-pca-ca-mca-and-visualization-r-software-and-data-mining) has several convenience functions for working with PCA.

## Mammal life history

```{r}
get_eig(z)
```

Eigenvalues are variances. Default R `print()` method returns standard deviations.


## Mammal life history

```{r}
print(z)
```


## Mammal life history

```{r}
fviz_eig(z, addlabels = TRUE)
```


## Mammal life history

```{r message=FALSE}
fviz_pca_var(z)
```


## Mammal life history

```{r message = FALSE}
fviz_pca_var(z, axes = c(2, 3))
```

## Sample size and other concerns

Suggested sample sizes vary:

- *n* = over 50
- *n*:*q* > 5:1 (often violated with genomic data)
- *n* and *n*:*q* are both large

All data:

- Numeric (continuous)
- No missing values


## Best case

- A small number of variables that can be used as surrogates for the larger set without too much loss of information
- Lower dimensional summary of a larger set of variables

PC1:

- Variables that combined account for the most variance
- For morphological data:
    - Can often be a proxy for "size"
    - Remaining PCs are "shape"


## Drawbacks

1. Lose the original variable identity
    - Interpretation can be a challenge
2. If centered and scaled (advised), you lose the original scale


## Hierarchy of GLMs

<center>
<img src="https://i.imgur.com/sdx4fMz.png" width="100%" />
</center>


## Approaches to continuous data

*Correlation*:

- 2 interchangeable variables

*Canonical correlation*

- *p* Continuous ~ *q* Continuous

*PCA*:

- 0 outcome variables (`~ .`)
- *q* possibly correlated variables

*Multiple regression*:

- 1 continuous outcome variable
- *q* predictors


## Multivariate (Gaussian) linear models

Directional:

$$\mbox{Outcome variables} \sim \mbox{Predictor variables}$$

1. "MLM": *p* Continuous ~ *q* Continuous
2. MANOVA and Hotelling's *T*^2^: *p* Continuous ~ *q* Categorical
3. MANCOVA: *p* Continuous ~ *q* Categorical + *r* Continuous
4. (Multiple) correspondence analysis: *p* Categorical ~ *q* Categorical

All of these create composite variables.

## Canonical correlation

Correlations of *linear combinations* of a set of multiple continuous variables with a separate set of multiple continuous variables.

$$\mbox{Multiple variables} \sim \mbox{Multiple variables}$$

- Weather variables vs. Growth variables
- Ecology variables vs. Urbanization variables
- Diversity variables vs. Landscape variables


## Wheat

> "Two identical 13.6-ton (500 bu.) parcels of Manitoba Northern wheat, variety Selkirk, were stored 183 cm deep in 2 similar and adjoining 305 cm X 333 cm wooden bins in a granary in Winnipeg during 1959-67. Two hundred-gram samples were collected monthly from fixed sampling sites in the bins." Sinha et al. [-@Sinha1969-mp]

Measure biotic (insects, fungi, mites) and abiotic (location, depth, temperature) factors.

- How do these correlate?


## Wheat

<center>
<img src="https://i.imgur.com/jJd74yv.png" width="50%" />
</center>


## MANOVA

Morant [-@Morant1923-co] published data on 32 skulls from Tibet:

```{r}
M <- read_excel("../data/Tibetan_Skulls.xlsx") %>%
  mutate(Origin = factor(Origin))
str(M)
```


## Tibetan skulls

<center>
<img src="https://i.imgur.com/a4DpQbL.png" width="80%" />
</center>


## Tibetan skulls

```{r echo=FALSE}
ggscatmat(M, 1:5, color = "Origin") +
  theme(text = element_text(size = 9),
        axis.text = element_text(size = 6),
        axis.text.x = element_text(angle = -90, vjust = 0.5))
```


## MANOVA: Tibetan skulls

```{r}
fm <- lm(cbind(Skull_Length, Skull_Width,
               Skull_Height, Face_Height,
               Face_Breadth) ~ Origin, data = M)
Anova(fm, type = "III")
```


## MANOVA vs DFA & Logistic Regression

MANOVA and Hotelling's *T*^2^: *p* Continuous ~ *q* Categorical

MANOVA creates a composite variable from the set of outcome variables and asks whether the groups differ.

  - Predict the composite from the group membership

DFA & Logistic Regression: *q* Categorical ~ *p* Continuous and/or *r* Categorical

DFA and Logistic Regression create a composite variable from the set of predictor variables and asks which variables best predict group membership.

  - Predict group membership from the composite scores


## Logistic regression vs. DFA

Both:

- Which variable(s) are good predictors of group membership
- Groups are known *a priori*
- Uses linear combinations of variables
- **Predict new observations**

Differences:

- Some assumptions
- Predictive ability (depends on specifics)
- See: [this stackexchange](https://stats.stackexchange.com/questions/95247/logistic-regression-vs-lda-as-two-class-classifiers)
- Ease of interpretation for different questions


## Penguin data

```{r}
library(palmerpenguins)
glimpse(penguins)
penguins <- penguins %>% 
  dplyr::select(species, bill_length_mm, bill_depth_mm,
         flipper_length_mm, body_mass_g) %>% 
  drop_na()
```


## Penguin data

```{r echo=FALSE}
ggscatmat(penguins, 2:5, color = "species") +
  theme(text = element_text(size = 9),
        axis.text = element_text(size = 6),
        axis.text.x = element_text(angle = -90, vjust = 0.5))
```


## DFA in R

Linear discriminant analysis:

- `lda()` with `Group ~ Predictors`

```{r}
penguin_lda <- lda(species ~ bill_length_mm + bill_depth_mm +
                     flipper_length_mm + body_mass_g,
                   data = penguins)
```


## Posterior prediction

```{r}
penguin_predict <- predict(penguin_lda, penguins[, 2:5])
penguin_predict$posterior %>% head(10)
```


## Classification

```{r}
(penguin_classify <- penguin_predict$class)
```


## Percent correct

Mean value of the correct classifications

```{r}
mean(penguin_classify == penguins$species)
```


## Confusion matrix

```{r}
table(Original = penguins$species, Predicted = penguin_classify)
```


## Predicting a new observation

```{r}
new_penguin <- tibble(bill_length_mm = 45,
                      bill_depth_mm = 15,
                      flipper_length_mm = 220,
                      body_mass_g = 5000)
```


## Predicting a new observation

```{r echo=FALSE, fig.height = 3.5}
p1 <- ggplot(penguins, aes(y = bill_length_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = "none")

p2 <- ggplot(penguins, aes(y = bill_depth_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = "none")

p3 <- ggplot(penguins, aes(y = flipper_length_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = c(0.65, 0.2),
        legend.box.background = element_rect(colour = "black"))

plot_grid(p1, p2, p3, ncol = 3)
```

```{r}
predict(penguin_lda, new_penguin)$posterior
```


## Predicting a new observation

```{r}
new_penguin <- tibble(bill_length_mm = 40,
                      bill_depth_mm = 12,
                      flipper_length_mm = 150,
                      body_mass_g = 2500)
```


## Predicting a new observation

```{r echo=FALSE, fig.height = 3.5}
p1 <- ggplot(penguins, aes(y = bill_length_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = "none")

p2 <- ggplot(penguins, aes(y = bill_depth_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = "none")

p3 <- ggplot(penguins, aes(y = flipper_length_mm, x = body_mass_g,
                     color = species)) +
  geom_point() +
  geom_point(data = new_penguin, color = "red", size = 3) +
  theme(legend.position = c(0.65, 0.2),
        legend.box.background = element_rect(colour = "black"))

plot_grid(p1, p2, p3, ncol = 3)
```

```{r}
predict(penguin_lda, new_penguin)$posterior
```


## References
