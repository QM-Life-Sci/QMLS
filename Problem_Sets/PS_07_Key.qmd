---
title: 'Problem Set 07'
author: "Your Name Here: Group X"
date: 'Last updated: `r Sys.Date()`'
format:
  html:
    toc: true
    number-depth: 3
    toc-location: left
    embed-resources: true
---

```{r}
#| label: setup
#| message: false
#| warning: false

# FIXME
library(tidyverse)
library(readxl)
library(cowplot)

library(knitr)
theme_set(theme_cowplot())
```

<!--
Datasets
  BrookTrout1.csv
  BrookTrout2.csv
  mammals.xlsx
-->

```{r}
#| echo: false
#| eval: false

# FIXME
# Generate data for question 1.
# P = 0.0495
n <- 15
set.seed(1064)
M1 <- tibble(
  Proportion_Surviving = c(rnorm(n, 0.28, 0.07),
                           rnorm(n, 0.242, 0.07)),
  Trout = factor(rep(c("Absent", "Present"), each = n))
)
write_csv(M1, path = "../data/BrookTrout1.csv")

ggplot(M1, aes(x = Trout, y = Proportion_Surviving)) +
  geom_point(position = position_jitter(width = 0.05), alpha = 0.5) +
  stat_summary(fun = mean, geom = "point", size = 3, color = "red") +
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.1,
               color = "red", size = 0.7) +
  labs(x = "Trout", y = "Survival", title = "BrookTrout1")

summary(lm(Proportion_Surviving ~ Trout, M1))

# P = 0.0536
n <- 15
set.seed(1064)
M2 <- tibble(
  Proportion_Surviving = c(rnorm(n, 0.282, 0.07),
                           rnorm(n, 0.245, 0.07)),
  Trout = factor(rep(c("Absent", "Present"), each = n))
)
write_csv(M2, path = "../data/BrookTrout2.csv")

ggplot(M2, aes(x = Trout, y = Proportion_Surviving)) +
  geom_point(position = position_jitter(width = 0.05), alpha = 0.5) +
  stat_summary(fun = mean, geom = "point", size = 3, color = "red") +
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.1,
               color = "red", size = 0.7) +
  labs(x = "Trout", y = "Survival", title = "BrookTrout2")

summary(lm(Proportion_Surviving ~ Trout, M2))
```

## Stalkies CI

## FIXME FIXME Confidence intervals

We will use the same data to explore confidence intervals.


### Activity

Calculate a ~95% confidence interval for each mean (cotton & corn groups). You can use the approximate formula discussed in lecture $\mu$ +/- 2 * SEM.

```{r}
# FIXME

stalk <- read_csv("../data/Stalkies.csv")

mean(stalk$eye_span[stalk$food_source == 'Cotton']) -
  2 * sd(stalk$eye_span[stalk$food_source == 'Cotton'] /
           sqrt(length(stalk$eye_span[stalk$food_source == 'Cotton'])))

mean(stalk$eye_span[stalk$food_source == 'Cotton']) +
  2 * sd(stalk$eye_span[stalk$food_source == 'Cotton'] /
           sqrt(length(stalk$eye_span[stalk$food_source == 'Cotton'])))

mean(stalk$eye_span[stalk$food_source == 'Corn']) -
  2 * sd(stalk$eye_span[stalk$food_source == 'Corn'] /
           sqrt(length(stalk$eye_span[stalk$food_source == 'Corn'])))

mean(stalk$eye_span[stalk$food_source == 'Corn']) +
  2 * sd(stalk$eye_span[stalk$food_source == 'Corn'] /
           sqrt(length(stalk$eye_span[stalk$food_source == 'Corn'])))

## We can write functions to do the SEM and approximate CI
## approx_CI returns a tibble with the lower and upper
## bounds of the CI. Note the use of group_modify() to 
## apply approx_CI() to each group.
SEM <- function(x) return(sd(x) / sqrt(length(x)))

approx_CI <- function(x, ...) {
  upper <- mean(x) + 2 * SEM(x)
  lower <- mean(x) - 2 * SEM(x)
  return(tibble(lower, upper))
}

stalk |> 
  group_by(food_source) |> 
  group_modify(~ approx_CI(.x$eye_span))
```

We can simulate what we would expect for a confidence interval if we set the true mean and standard deviation for our population. Knowing these true values allows us to draw random data from a true underlying distribution. 

For this simulation, we could assume our estimated means and standard deviations are representative of the population parameters. For any empirical dataset, will you ever know the true mean and standard deviation?

> No. It will always be an estimate. You expect it to get closer to the truth the higher the sample size but it is still an estimate.

Let's set up our simulation. Do the following:

1. Generate simulated data from a normal distribution matching the number of observations, mean, and standard deviation (note sd not SEM) for each group.
2. Calculate the mean for each group and the upper and lower bounds of the confidence interval.
3. Put these values into a `tibble`
4. Repeat this 1000 times
5. Calculate the proportion of times the mean for each group falls within the confidence interval


```{r}
# FIXME
# Note: need to use list() below to assign to a row of a tibble.

set.seed(648273)
niter <- 1000

mu.corn <- mean(stalk$eye_span[stalk$food_source == 'Corn'])
mu.cotton <- mean(stalk$eye_span[stalk$food_source == 'Cotton'])
sd.corn <- sd(stalk$eye_span[stalk$food_source == 'Corn'])
sd.cotton <- sd(stalk$eye_span[stalk$food_source == 'Cotton'])
n.corn <- length(stalk$eye_span[stalk$food_source == 'Corn'])
n.cotton <- length(stalk$eye_span[stalk$food_source == 'Cotton'])

corn.cis <- tibble(mm = numeric(length = niter),
                   ll = numeric(length = niter),
                   uu = numeric(length = niter))
cotton.cis <- tibble(mm = numeric(length = niter),
                     ll = numeric(length = niter),
                     uu = numeric(length = niter))

for (jj in 1:niter) {
  corn.s <- rnorm(n.corn, mu.corn, sd.corn)
  cotton.s <- rnorm(n.cotton, mu.cotton, sd.cotton)

  corn.cis[jj, ] <- list(
    mean(corn.s),
    mean(corn.s) - 2 * (sd(corn.s) / sqrt(n.corn)),
    mean(corn.s) + 2 * (sd(corn.s) / sqrt(n.corn)))
  cotton.cis[jj, ] <- list(
    mean(cotton.s),
    mean(cotton.s) - 2 * (sd(cotton.s) / sqrt(n.cotton)),
    mean(cotton.s) + 2 * (sd(cotton.s) / sqrt(n.cotton)))
}

(sum(mu.corn < corn.cis$ll) + sum(mu.corn > corn.cis$uu)) / niter
(sum(mu.cotton < cotton.cis$ll) + sum(mu.cotton > cotton.cis$uu)) / niter
```

What do you conclude based on this simulation?

> The true mean is within about 95% of the resampled confidence intervals.

Is it correct to say that there is a 95% probability that the true mean is within a 95% confidence interval? Why or why not? Think about what you just showed with your simulation.

> No, it is not correct. CIs tell us about a range of values that we have some confidence in for *future* samples. This is a nuanced but very important point about confidence intervals.




## Brook trout

You and a collaborator are studying the role of an introduced species (brook trout, *Salvelinus fontinalis*) on survival of native Chinook salmon (*Oncorhynchus tshawytscha*). Because you are located at different universities, you each have a different set of streams to sample. Independently, each of you measures survivorship of Chinook salmon in 15 streams that have introduced brook trout and in 15 streams that are free of brook trout. To estimate survivorship (percent alive), you released an equal number of tagged juvenile Chinook salmon into each stream and then resampled over the course of a year.

The two data files are:

1. Your data: `BrookTrout1.csv`
2. You collaborator's data: `BrookTrout2.csv`

### Activity

Load each of the files into R.

```{r}
# FIXME
BT1 <- read_csv("../data/BrookTrout1.csv")
BT2 <- read_csv("../data/BrookTrout2.csv")
```

Make a plot of each data set using the same ggplot code you used to visualized the stalk-eyed fly data in the previous problem set. Plot the raw data (points) for the two groups (trout `Absent` and `Present`). For the points, include jitter and transparency. Include means and standard errors plotted in a different color.

Assign each plot to an R object (we like to use `p1` and `p2` for plot 1 and plot 2). Then use the `plot_grid()` function in the `cowplot` package to stack the two plots on top of one another. You code for the last step will look something like: `plot_grid(p1, p2, nrow = 2)`. If you used `ncol = 2` instead, you would get plots side by side.

`plot_grid()` is a really handy function for combining different ggplots into a single figure. It has facilities for adding labels ("a.", "b.", etc.), changing the spacing between plots, and much more. For us, it is indispensable for making publication-ready figures.

```{r}
# FIXME
p1 <- ggplot(BT1, aes(x = Trout, y = Proportion_Surviving)) +
  geom_point(position = position_jitter(width = 0.05), alpha = 0.5) +
  stat_summary(fun = mean, geom = "point", size = 3, color = "red") +
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.1,
               color = "red", size = 0.7) +
  labs(x = "Trout", y = "Survival", title = "My Data")
p2 <- ggplot(BT2, aes(x = Trout, y = Proportion_Surviving)) +
  geom_point(position = position_jitter(width = 0.05), alpha = 0.5) +
  stat_summary(fun = mean, geom = "point", size = 3, color = "red") +
  stat_summary(fun.data = mean_se, geom = "errorbar", width = 0.1,
               color = "red", size = 0.7) +
  labs(x = "Trout", y = "Survival", title = "My Collaborator's Data")

plot_grid(p1, p2, nrow = 2)
```

Considering the plot you just made, do you think that mean survivorship is significantly different in the Absent/Present treatment groups? Do you think that the two sets of data are different from one another?

> There is much variability, but survival does appear to be higher in the trout absent treatment group. The SEMs don't overlap it seems. There might be a significant different between treatments. The patterns look the same across locations.

First, fit a linear model to **your data** (from `BrookTrout1.csv`), in which the proportion of salmon surviving is modeled by presence or absence of trout. Print out the `summary()` of this linear model.

```{r}
# FIXME
fm1 <- lm(Proportion_Surviving ~ Trout, BT1)
summary(fm1)
```

Note that, although these data are proportions (and thus bounded by 0 and 1), the observed values are normally distributed enough within in group to analyze with a regular linear model. If the values are clumped either close to 0 or to 1, then a different model is likely needed. For example, if you are trying to analyze student exam scores, you often run into problems because they are often heavily skewed and often run up against 1. Technically, it is the residuals that need to be normally distributed, but if the predictors are not normal then often the residuals won't be either.

All that being said, how do you interpret the output of the linear model? Is there is significant difference in mean survival between the groups? Choose the $\alpha$-level you feel is appropriate.

> At $\alpha = 0.05$, there is a significant difference between the groups with lower salmon survivorship in the presence of trout (mean difference = 0.055 or 5.5%; P = 0.0495).

Now fit the same model to your **collaborator's data** (from `BrookTrout2.csv`).

```{r}
# FIXME
fm2 <- lm(Proportion_Surviving ~ Trout, BT2)
summary(fm2)
```

What do you conclude about your collaborator's data?

> $P = 0.054$ here. So we conclude, based on $\alpha = 0.05$, that there is not a significant difference in means.

How do you feel about the differing results of these two studies?

> They are nearly identical (5.5% vs. 5.3% difference in survivorship), but the arbitrarily chosen $\alpha$ level leads you to two different conclusions.

Because you have data on individual-level survival of salmon, which were used to make calculate the survival proportions, you might be tempted to use a [$\chi^2$ test of association](http://www.stat.yale.edu/Courses/1997-98/101/chisq.htm) on the combined data. That table would look something like this table, where you have aggregated counts for numbers of surviving salmon in the presence or absence of trout:

```{r echo=FALSE}
M <- tibble::tibble(Survival = c("Survived", "Did not survive"),
                    `Trout absent` = c(946, 3470),
                    `Trout present` = c(919, 3876))
knitr::kable(M)
```

Thinking about the assumptions of the statistical tests that we have learned, why is combining all the data for this test not appropriate? 

> Your data and the data from your collaborator are from completely different streams, which is good, but streams differ from onr another (water chemistry, abundance of food, presence of other predators, etc.). And because we have many salmon from the same stream, the observations (whether an individual salmon juvenile was alive or not) are not independent of one another. This non-independence means that all the salmon in a stream are more likely to have the same survival probability. Streams add a level of structure to the data. Later in the course, we will cover multilevel and logistic regression, which are (separately or combined) another way to correctly model survival, accounting for possible different survival rates in both streams.


## Exact confidence intervals

We ultimately want to calculate exact confidence intervals for the proportion surviving in your data. First, we want to explore the adjustment for degrees of freedom (the critical *t*-value). Originally, we showed you how to calculate the approximate CI, using twice the standard error of the mean. Using 2 will result in too narrow CIs for small degrees of freedom and too wide CIs for large degrees of freedom.

### Activity

Make a plot of the critical value for a *t*-distribution for the 0.975 quantile (upper tail of 2.5%) where the degrees of freedom ranges from 2 to 100. First set up the vector of degrees of freedom (don't call it df, because that is the name of an R function: the density of an *F*-distribution). Then calculate the critical values. Make a tibble and plot the results. Then add a differently colored horizontal line at the critical value for the 0.975 quantile for a standard normal distribution. Note that the critical value approaches the asymptotic limit as the degrees of freedom increases.

```{r}
# FIXME
CVs <- tibble(dfs = seq(2, 100, by = 1),
              crit_vals = qt(0.975, df = dfs,
                             lower.tail = TRUE))

CVs |>
  ggplot(aes(dfs, crit_vals)) +
  geom_line() +
  geom_hline(yintercept = qnorm(0.025, lower.tail = FALSE),
             color = "blue") +
  labs(x = "Degrees of Freedom", y = "Critical value (0.975)")
```

Now calculate the exact 95% confidence intervals for the two groups in your data (from `BrookTrout1.csv`). Either pull the critical value from your tibble or just calculate anew.

```{r}
# Let's just adapt the approximate CI function from
# Problem Set 6. We will make it a little more flexible
# by allowing us to specify the desired interval with
# `prob`. Default to 95% CI.
SEM <- function(x) return(sd(x) / sqrt(length(x)))

exact_CI <- function(x, prob = 0.95) {
  t_crit <- qt((1 - prob) / 2,
               df = length(x) - 1,
               lower.tail = FALSE)
  Upper <- mean(x) + t_crit * SEM(x)
  Lower <- mean(x) - t_crit * SEM(x)
  return(tibble(Lower, Mean = mean(x), Upper))
}

CIs <- BT1 |> 
  group_by(Trout) |> 
  group_modify(~ exact_CI(.x$Proportion_Surviving))

knitr::kable(CIs)
```

